<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zhengbo Wang (王政博) </title> <meta name="author" content="Zhengbo Wang (王政博)"> <meta name="description" content="My personal academic website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ZW-logos_3.jpeg?7b33a733d6de67e377a6f7feba6d1ac5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mrflogs.github.io/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%7A%68%65%6E%67%62%6F%77%61%6E%67@%6D%61%69%6C.%75%73%74%63.%65%64%75.%63%6E" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=u54ct3gAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/mrflogs" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://twitter.com/burntto33688196" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Zhengbo</span> Wang (王政博) </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zb_prof_pic-480.webp 480w,/assets/img/zb_prof_pic-800.webp 800w,/assets/img/zb_prof_pic-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/zb_prof_pic.jpg?371b1250ccf0f135f043f316485db392" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="zb_prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <style>.infoblock{background-color:#ffe;*/}.blockcontent{border:1px solid silver;color:black;padding:.3em .5em}</style> <p>Hi there (｡･∀･)ﾉﾞ.</p> <p>I am a Ph.D. student at <a href="https://www.ustc.edu.cn/" rel="external nofollow noopener" target="_blank">University of Science and Technology of China</a>, under the supervision of <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Prof. Tieniu Tan</a> and co-supervision of <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Prof. Jian Liang</a>, <a href="https://rhe-web.github.io/" rel="external nofollow noopener" target="_blank">Prof. Ran He</a> and <a href="http://staff.ustc.edu.cn/~zlwang/index_en.html" rel="external nofollow noopener" target="_blank">Prof. Zilei Wang</a>. Before that, I earned my BS in Electronic Information Engineering from <a href="https://www.ustc.edu.cn/" rel="external nofollow noopener" target="_blank">University of Science and Technology of China</a> in 2021.</p> <p>My current research focuses on developing safe, robust, and efficient fine-tuning methods for vision-language models and large language models. See also my <a href="/publications/"> list of publications. </a></p> <p>You can download <a href="/assets/pdf/2024_resume.pdf">my CV here</a>.</p> <p>Email is the best way to reach me: <a href="mailto:zhengbowang@mail.ustc.edu.cn">zhengbowang@mail.ustc.edu.cn</a>. Feel free to send me a message!</p> </div> <h2> Education </h2> <style>.education-block{display:flex;align-items:center;max-width:800px;margin:0 auto;margin-bottom:20px}.education-logo{flex:0 0 100px;margin-right:20px}.education-logo img{width:100%}</style> <div class="education-block"> <div class="education-logo"> <img src="assets/img/Logo_of_University_of_Science_and_Technology_of_China.svg.png" alt="USTC Logo"> </div> <div class="education-details"> <b> University of Science and Technology of China </b> <br> School of Information Science and Technology <br> PhD student, 2021 - present </div> </div> <div class="education-block"> <div class="education-logo"> <img src="assets/img/Logo_of_University_of_Science_and_Technology_of_China.svg.png" alt="USTC Logo"> </div> <div class="education-details"> <b> University of Science and Technology of China </b> <br> School of Information Science and Technology <br> Undergraduate, 2017 - 2021 </div> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May 02, 2024</th> <td> Two of our paper have been accepted at ICML 2024! 🤯🤯🤯 </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 15, 2024</th> <td> Our paper <a href="https://openreview.net/forum?id=Js5PJPHDyY" rel="external nofollow noopener" target="_blank">“A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation”</a> has been accepted by ICLR 2024! Code is available now! 😊😊 </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 06, 2023</th> <td> We achieved 1st place in the UniDA track of the GeoNet competition at ICCV23! 😀😀 </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 14, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2307.07397" rel="external nofollow noopener" target="_blank">“Improving Zero-Shot Generalization for CLIP with Synthesized Prompts”</a> has been accepted by ICCV 2023! 🥳🥳🥳 </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 16, 2023</th> <td> Our paper <a href="https://arxiv.org/abs/2303.09849" rel="external nofollow noopener" target="_blank">“Exploiting Semantic Attributes for Transductive Zero-Shot Learning”</a> has been accepted by ICASSP 2023! It’s my first paper! 😁😁😁 </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICLR</abbr> </div> <div id="wang2023hard" class="col-sm-8"> <div class="title">A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation</div> <div class="author"> <em>Zhengbo Wang</em>, <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Jian Liang</a>, <a href="https://tomsheng21.github.io/" rel="external nofollow noopener" target="_blank">Lijun Sheng</a>, <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Ran He</a>, <a href="https://vim.ustc.edu.cn/main.htm" rel="external nofollow noopener" target="_blank">Zilei Wang</a>, and <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tieniu Tan</a> </div> <div class="periodical"> <em>In The Twelfth International Conference on Learning Representations</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=Js5PJPHDyY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2024_ICLR_hard2beat.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mrflogs/ICLR24" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://zhengbo.wang/ICLR24" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP’s performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes’ formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at https://github.com/mrflogs/ICLR24.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023hard</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Zhengbo and Liang, Jian and Sheng, Lijun and He, Ran and Wang, Zilei and Tan, Tieniu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICCV</abbr> </div> <div id="wang2023improving" class="col-sm-8"> <div class="title">Improving Zero-Shot Generalization for CLIP with Synthesized Prompts</div> <div class="author"> <em>Zhengbo Wang</em>, <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Jian Liang</a>, <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Ran He</a>, Nan Xu , <a href="https://vim.ustc.edu.cn/main.htm" rel="external nofollow noopener" target="_blank">Zilei Wang</a>, and <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tieniu Tan</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2307.07397" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2023_ICCV_SHIP.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mrflogs/SHIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf’s law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \textbfSynt\textbfHes\textbfIzed \textbfPrompts (\textbfSHIP) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and synthesized features. Extensive experiments on base-to-new generalization, cross-dataset transfer learning, and generalized zero-shot learning demonstrate the superiority of our approach. The code is available at \urlhttps://github.com/mrflogs/SHIP.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023improving</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Zero-Shot Generalization for CLIP with Synthesized Prompts}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Zhengbo and Liang, Jian and He, Ran and Xu, Nan and Wang, Zilei and Tan, Tieniu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3032--3042}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICASSP</abbr> </div> <div id="wang2023exploiting" class="col-sm-8"> <div class="title">Exploiting Semantic Attributes for Transductive Zero-Shot Learning</div> <div class="author"> <em>Zhengbo Wang</em>, <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Jian Liang</a>, <a href="https://vim.ustc.edu.cn/main.htm" rel="external nofollow noopener" target="_blank">Zilei Wang</a>, and <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tieniu Tan</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2303.09849" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2023_ICASSP.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mrflogs/icassp23" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Zero-shot learning (ZSL) aims to recognize unseen classes by generalizing the relation between visual features and semantic attributes learned from the seen classes. A recent paradigm called transductive zero-shot learning further leverages unlabeled unseen data during training and has obtained impressive results. These methods always synthesize unseen features from attributes through a generative adversarial network to mitigate the bias towards seen classes. However, they neglect the semantic information in the unlabeled unseen data and thus fail to generate high-fidelity attribute-consistent unseen features. To address this issue, we present a novel transductive ZSL method that produces semantic attributes of the unseen data and imposes them on the generative process. In particular, we first train an attribute decoder that learns the mapping from visual features to semantic attributes. Then, from the attribute decoder, we obtain pseudo-attributes of unlabeled data and integrate them into the generative model, which helps capture the detailed differences within unseen classes so as to synthesize more discriminative features. Experiments on five standard benchmarks show that our method yields state-of-the-art results for zero-shot learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023exploiting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploiting Semantic Attributes for Transductive Zero-Shot Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Zhengbo and Liang, Jian and Wang, Zilei and Tan, Tieniu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICCVW</abbr> </div> <div id="sheng2023self" class="col-sm-8"> <div class="title">Self-training solutions for the ICCV 2023 GeoNet Challenge</div> <div class="author"> <a href="https://tomsheng21.github.io/" rel="external nofollow noopener" target="_blank">Lijun Sheng</a> , <em>Zhengbo Wang</em>, and <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Jian Liang</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2311.16843</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://liangjian.xyz/assets/paper/iccvw23.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2023_ICCVW_GeoNet.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/tim-learn/GeoNet23_casia_tim" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sheng2023self</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-training solutions for the ICCV 2023 GeoNet Challenge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sheng, Lijun and Wang, Zhengbo and Liang, Jian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2311.16843}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Preprint</abbr> </div> <div id="liang2023towards" class="col-sm-8"> <div class="title">Towards Realistic Unsupervised Fine-tuning with CLIP</div> <div class="author"> <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Jian Liang</a>, <a href="https://tomsheng21.github.io/" rel="external nofollow noopener" target="_blank">Lijun Sheng</a> , <em>Zhengbo Wang</em>, <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Ran He</a>, and <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tieniu Tan</a> </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/xxx" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels. To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICML</abbr> </div> <div id="wang2024connecting" class="col-sm-8"> <div class="title">Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models</div> <div class="author"> <em>Zhengbo Wang</em>, <a href="https://liangjian.xyz/" rel="external nofollow noopener" target="_blank">Jian Liang</a>, <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Ran He</a>, <a href="https://vim.ustc.edu.cn/main.htm" rel="external nofollow noopener" target="_blank">Zilei Wang</a>, and <a href="https://scholar.google.com/citations?user=W-FGd_UAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Tieniu Tan</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2402.04050</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2402.04050" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2024_ArXiv_CraFT.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model’s parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbfCollabo\textbfrative \textbfFine-\textbfTuning (\textbfCraFT) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62% compared to the white-box method.</p> </div> </div> </div> </li> </ol> </div> <script type="text/javascript" src="https://www.freevisitorcounters.com/en/home/counter/1161821/t/5"></script> <br> <img src="https://count.getloli.com/get/@zhengbo.wang?theme=gelbooru" alt="zhengbo.wang"> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Zhengbo Wang (王政博). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: May 07, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>